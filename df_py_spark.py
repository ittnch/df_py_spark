# -*- coding: utf-8 -*-
"""df_py_spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14RCb-5IbB_qZx-eilex7DgrRdxW0N5pf
"""

!pip install findspark

!pip install pyspark

import pyspark

from pyspark.sql import SparkSession

import findspark
findspark.init()
from pyspark.sql import SparkSession

findspark.find()

# создание SparkSession
spark=SparkSession.builder.getOrCreate()

path='/content/train.csv'

# чтение данных
df = spark.read.csv(path, inferSchema = True, header = True)

df.show()

# возвращение схемы данных
df.printSchema()

# список кортежей с именами столбцов и типами данных
df.dtypes

# в виде списка
df.head(3)

# показываем первую строку данных
df.first()

# общее количество строк датасета
df.count()

# выводим названия столбцов
df.columns

LotFrontage = df.select('LotFrontage').show(5)

# Выбор одного столбца
df.select('Id').show(5)

#вычисление значения столбца
df.withColumn('Avg_liv_lot', df['BsmtFullBath']/df['LotArea']).withColumn('MSSubClass+10', df['MSSubClass']+10).show()

# Выбор нескольких столбцов
df.select(['YearBuilt', 'YearRemodAdd', 'YrSold','SalePrice']).show(5)

# выбор года продажи от 2008 до 2010
from pyspark.sql.functions import col, lit

df.filter( (col('YrSold') >= lit('2008')) & (col('YrSold') <= lit('2010')) ).show(5)

# замена категориальной переменной значения на бинарный
from pyspark.sql import functions as f
df.select('PavedDrive', 'Id', f.when(df.PavedDrive == 'Y', 1).otherwise(0)
).show(5)

from pyspark.sql.functions import col,lit, to_timestamp, concat_ws, date_format
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.sql import functions as F
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import StringIndexer

df.show()

# Создаем StringIndexer для столбца MoSold
indexer = StringIndexer(inputCol="MoSold", outputCol="MoSoldIndex")

# Создаем OneHotEncoder для столбца MoSoldIndex
encoder = OneHotEncoder(inputCol="MoSoldIndex", outputCol="MoSoldVec")

# Подсчет количества пропущенных значений
# Список признаков для проверки
feature_list = ['LotArea', "OverallCond", "TotalBsmtSF", "1stFlrSF", "2ndFlrSF", "FullBath", "BedroomAbvGr",
                   "TotRmsAbvGrd", "GarageCars", "WoodDeckSF", "OpenPorchSF", "PoolArea", "YearBuilt"]

for feature in feature_list:

    values_miss = df.where(col(feature).isNull()).count()
    print(f"Количество пропущенных значений у признака '{feature}': {values_miss}")

# Выберем исходные числовые переменные
numeric_columns = ["YrSold","LotArea", "OverallCond", "TotalBsmtSF", "1stFlrSF", "2ndFlrSF", "FullBath", "BedroomAbvGr",
                   "TotRmsAbvGrd", "GarageCars", "WoodDeckSF", "OpenPorchSF", "PoolArea", "YearBuilt"]

# Создание вектора признаков
assembler = VectorAssembler(inputCols=numeric_columns, outputCol="features")
data_assembled = assembler.transform(df)

# Добавляем целевую переменную
data_assembled = data_assembled.withColumn("label", data_assembled.SalePrice)

# Масштабирование числовых переменных
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(data_assembled)
data_scaled = scaler_model.transform(data_assembled)

from pyspark.ml.regression import LinearRegression

# Разделяем данные на обучающую и тестовую выборки
(trainingData, testData) = data_scaled.randomSplit([0.8, 0.2])

# Создаем модель линейной регрессии
lr = LinearRegression(maxIter=10, regParam=0.01, elasticNetParam=0.0)

# Обучаем модель
lrModel = lr.fit(trainingData)

# Получаем прогнозы на тестовых данных
predictions = lrModel.transform(testData)

# Выводим результаты
predictions.select("prediction", "label").show()

house_results=lrModel.evaluate(trainingData)

print('Rsquared Error :',house_results.r2)
#R2 значение показывает точность модели is 73%

test_results = lrModel.evaluate(testData)
print("MSE: {}".format(test_results.meanSquaredError))

path_test='/content/test.csv'
df_test = spark.read.csv(path_test, inferSchema = True, header = True)
df_test.show()

df_test.count()

df_test.dtypes

# Преобразование столбцов TotalBsmtSF и GarageCars к типу integer
df_test = df_test.withColumn("TotalBsmtSF", col("TotalBsmtSF").cast("integer"))
df_test = df_test.withColumn("GarageCars", col("GarageCars").cast("integer"))

# Проверка изменений
df_test.printSchema()

# Создание словаря с новыми именами столбцов
new_column_names = {
    "YrSold": "NewYrSold",
    "LotArea": "NewLotArea",
    "OverallCond": "NewOverallCond",
    "TotalBsmtSF": "NewTotalBsmtSF",
    "1stFlrSF": "New1stFlrSF",
    "2ndFlrSF": "New2ndFlrSF",
    "FullBath": "NewFullBath",
    "BedroomAbvGr": "NewBedroomAbvGr",
    "TotRmsAbvGrd": "NewTotRmsAbvGrd",
    "GarageCars": "NewGarageCars",
    "WoodDeckSF": "NewWoodDeckSF",
    "OpenPorchSF": "NewOpenPorchSF",
    "PoolArea": "NewPoolArea",
    "YearBuilt": "NewYearBuilt"
}

# Переименование столбцов в DataFrame
for old_col, new_col in new_column_names.items():
    df_test = df_test.withColumnRenamed(old_col, new_col)

# Проверка изменений
df_test.show()

# исходные числовые переменные
numeric_columns_test = ["NewYrSold","NewLotArea", "NewOverallCond", "NewTotalBsmtSF", "New1stFlrSF", "New2ndFlrSF", "NewFullBath", "NewBedroomAbvGr",
                   "NewTotRmsAbvGrd", "NewGarageCars", "NewWoodDeckSF", "NewOpenPorchSF", "NewPoolArea", "NewYearBuilt"]

# Создание вектора признаков для новых данных
assembler_new = VectorAssembler(inputCols=numeric_columns_test, outputCol="features")
new_data = assembler_new.transform(df_test)

# Применение масштабирования к новым данным
new_data_scaled = scaler_model.transform(new_data)

# Получение прогнозов на новых данных
predictions_new = lrModel.transform(new_data_scaled)

predictions_new.show()

